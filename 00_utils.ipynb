{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Utils\n",
    "\n",
    "> Basic utility functions across sac_stac."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import zipfile\n",
    "import time\n",
    "from glob import glob\n",
    "from urllib.parse import urlparse\n",
    "import gc\n",
    "import shutil\n",
    "\n",
    "import boto3\n",
    "import gdal\n",
    "from pystac import STAC_IO\n",
    "\n",
    "from sedas_pyapi.sedas_api import SeDASAPI\n",
    "from sedas_pyapi.bulk_download import SeDASBulkDownload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sedas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably best if we start by logging into sedas to be able to access datasets in their current format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def sedas_client():\n",
    "    \"Log into sedas api.\"\n",
    "    sedas = SeDASAPI(os.getenv('SEDAS_USERNAME'), os.getenv('SEDAS_PWD'))\n",
    "    sedas.base_url=\"https://geobrowsertest.satapps.org/api/\"\n",
    "    return sedas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sedas_pyapi.sedas_api.SeDASAPI at 0x7f0a49feabd0>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sedas_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "should inc. test for env vars...?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we'll want to find datasets from different *Collections*, which sedas expects as *groups*. Will eventually **need to add test endpoint** to be able to access NovaSAR datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def sedas_get_collections():\n",
    "    sedas = sedas_client()\n",
    "    result_groups = sedas.list_sensor_groups()\n",
    "    groups = []\n",
    "    for i in range(0, len(result_groups)):\n",
    "        groups.append(result_groups[i]['name'])\n",
    "    return f\"Available groups are: {', '.join(groups)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Available groups are: Cosmo-SkyMed, SPOT, Pleiades, S1, S2, AIRSAR'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sedas_get_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def sedas_find_datasets(wkt, startDate, endDate, collection):\n",
    "    optical_groups = ['S2','Pleiades','SPOT']\n",
    "    sar_groups = ['S1','Cosmo-SkyMed','AIRSAR']\n",
    "    sedas = sedas_client()\n",
    "    if collection in optical_groups: \n",
    "        res = sedas.search_optical(wkt, startDate, endDate, source_group=collection) \n",
    "    elif collection in sar_groups:\n",
    "        res = sedas.search_sar(wkt, startDate, endDate, source_group=collection)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example for a small area over Oxford we can find..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>productId</th>\n",
       "      <th>supplierId</th>\n",
       "      <th>type</th>\n",
       "      <th>satelliteName</th>\n",
       "      <th>instrumentName</th>\n",
       "      <th>modeName</th>\n",
       "      <th>sensorType</th>\n",
       "      <th>sensorResolution</th>\n",
       "      <th>coordinatesWKT</th>\n",
       "      <th>start</th>\n",
       "      <th>...</th>\n",
       "      <th>area</th>\n",
       "      <th>aoiCoveragePercent</th>\n",
       "      <th>usefulAreaPercent</th>\n",
       "      <th>cloudCoveragePercent</th>\n",
       "      <th>productType</th>\n",
       "      <th>latency</th>\n",
       "      <th>ql</th>\n",
       "      <th>thumbnail</th>\n",
       "      <th>vendorSpecific</th>\n",
       "      <th>downloadUrl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c0ec9a5f12356e87bc910ddbc49dbb76</td>\n",
       "      <td>Pleiades_UKSA396_SO18034616-96-01_DS_PHR1B_201...</td>\n",
       "      <td>ARCHIVE</td>\n",
       "      <td>Pleiades-1B</td>\n",
       "      <td>MS/PAN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>Optical</td>\n",
       "      <td>2.0</td>\n",
       "      <td>POLYGON((-1.654728 51.309517,-1.345414 51.3081...</td>\n",
       "      <td>2018-10-24T11:17:22Z</td>\n",
       "      <td>...</td>\n",
       "      <td>4.825125e+08</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>L3</td>\n",
       "      <td>Standard</td>\n",
       "      <td>https://geobrowser.satapps.org/archiveql/aeweb...</td>\n",
       "      <td>https://sedasdm.satapps.org/qls/qlmgr.php?scen...</td>\n",
       "      <td>{'property': 'vendorSpecific', 'Filehash': 'cf...</td>\n",
       "      <td>https://sedasdm.satapps.org/datamgr/datamgr.ph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>94cc7887414be0912de7ca44288f79da</td>\n",
       "      <td>Pleiades_UKSA174_SO18034614-74-01_DS_PHR1A_201...</td>\n",
       "      <td>ARCHIVE</td>\n",
       "      <td>Pleiades-1A</td>\n",
       "      <td>MS/PAN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>Optical</td>\n",
       "      <td>2.0</td>\n",
       "      <td>POLYGON((-2.153852 51.603313,-1.841533 51.6033...</td>\n",
       "      <td>2018-09-29T11:10:08Z</td>\n",
       "      <td>...</td>\n",
       "      <td>1.565260e+08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>L3</td>\n",
       "      <td>Standard</td>\n",
       "      <td>https://geobrowser.satapps.org/archiveql/aeweb...</td>\n",
       "      <td>https://sedasdm.satapps.org/qls/qlmgr.php?scen...</td>\n",
       "      <td>{'property': 'vendorSpecific', 'Filehash': '4b...</td>\n",
       "      <td>https://sedasdm.satapps.org/datamgr/datamgr.ph...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          productId  \\\n",
       "0  c0ec9a5f12356e87bc910ddbc49dbb76   \n",
       "1  94cc7887414be0912de7ca44288f79da   \n",
       "\n",
       "                                          supplierId     type satelliteName  \\\n",
       "0  Pleiades_UKSA396_SO18034616-96-01_DS_PHR1B_201...  ARCHIVE   Pleiades-1B   \n",
       "1  Pleiades_UKSA174_SO18034614-74-01_DS_PHR1A_201...  ARCHIVE   Pleiades-1A   \n",
       "\n",
       "  instrumentName modeName sensorType  sensorResolution  \\\n",
       "0         MS/PAN    0.000    Optical               2.0   \n",
       "1         MS/PAN    0.000    Optical               2.0   \n",
       "\n",
       "                                      coordinatesWKT                 start  \\\n",
       "0  POLYGON((-1.654728 51.309517,-1.345414 51.3081...  2018-10-24T11:17:22Z   \n",
       "1  POLYGON((-2.153852 51.603313,-1.841533 51.6033...  2018-09-29T11:10:08Z   \n",
       "\n",
       "   ...          area aoiCoveragePercent  usefulAreaPercent  \\\n",
       "0  ...  4.825125e+08                1.0                5.0   \n",
       "1  ...  1.565260e+08                2.0               22.0   \n",
       "\n",
       "   cloudCoveragePercent  productType   latency  \\\n",
       "0                   0.0           L3  Standard   \n",
       "1                   0.0           L3  Standard   \n",
       "\n",
       "                                                  ql  \\\n",
       "0  https://geobrowser.satapps.org/archiveql/aeweb...   \n",
       "1  https://geobrowser.satapps.org/archiveql/aeweb...   \n",
       "\n",
       "                                           thumbnail  \\\n",
       "0  https://sedasdm.satapps.org/qls/qlmgr.php?scen...   \n",
       "1  https://sedasdm.satapps.org/qls/qlmgr.php?scen...   \n",
       "\n",
       "                                      vendorSpecific  \\\n",
       "0  {'property': 'vendorSpecific', 'Filehash': 'cf...   \n",
       "1  {'property': 'vendorSpecific', 'Filehash': '4b...   \n",
       "\n",
       "                                         downloadUrl  \n",
       "0  https://sedasdm.satapps.org/datamgr/datamgr.ph...  \n",
       "1  https://sedasdm.satapps.org/datamgr/datamgr.ph...  \n",
       "\n",
       "[2 rows x 22 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = sedas_find_datasets(\"POLYGON((-1.91 51.81,-1.15 51.81,-1.15 51.50,-1.91 51.50,-1.91 51.81))\", \n",
    "                             \"2000-01-01T00:00:00Z\", \n",
    "                             \"2020-10-27T00:00:00Z\",\n",
    "                             \"Pleiades\"\n",
    "                            )\n",
    "pd.DataFrame(result['products']).head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do a single download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def sedas_download(sedas_res_dicts, down_dir, sedas=None):\n",
    "    \"Use product dicts list to download into a dir.\"\n",
    "    if not sedas:\n",
    "        sedas = sedas_client()\n",
    "    downloader = SeDASBulkDownload(sedas, down_dir, parallel=2)\n",
    "    downloader.add(sedas_res_dicts)\n",
    "    print(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} Downloading\")\n",
    "    while not downloader.is_done():\n",
    "        time.sleep(5)\n",
    "    downloader.shutdown()\n",
    "    print(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} Downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tbd example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def sedas_extract(down_zip, scene_dir):\n",
    "    print(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} Extracting {down_zip}\")\n",
    "    with zipfile.ZipFile(down_zip, 'r') as zip_ref:\n",
    "        zip_ref.extractall(scene_dir)\n",
    "    if os.path.exists(scene_dir):\n",
    "        os.remove(down_zip)\n",
    "    print(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} Extracted to {scene_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tbd example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud-Optimised Formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have access to the datasets, we will want to convert raster images of any format into a default COG format. (It helps that COGs are now an official [gdal driver](https://gdal.org/drivers/raster/cog.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def convert2cog(img_path, cog_path, band):\n",
    "    \"\"\"\n",
    "    Convert gdal raster into COG with default settings.\n",
    "    Considering lzw compression.\n",
    "    See https://www.cogeo.org/developers-guide.html.\n",
    "    \"\"\"\n",
    "    # translate into new cog file\n",
    "    kwargs = {\n",
    "        'format': 'COG',\n",
    "#         'creationOptions' : ['COMPRESS=LZW'],\n",
    "        'bandList': [band]\n",
    "    }\n",
    "    print(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} Starting conversion: {img_path}.\")\n",
    "    ds = gdal.Translate(cog_path, img_path, **kwargs)\n",
    "    ds = None\n",
    "    print(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} Conversion complete: {os.path.exists(cog_path)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def mosaic2cog(imgs, out_cog, band):\n",
    "    \"\"\"\n",
    "    Mosaic imgs into cog.\n",
    "    Usual vrt assumptions.\n",
    "    \"\"\"\n",
    "    tmp_vrt = f\"{out_cog[:-4]}_mosaic.tif\"\n",
    "    # mosaic into vrt\n",
    "    print(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} Mosaicing band {band} imgs: {imgs}\")\n",
    "    ds = gdal.BuildVRT(tmp_vrt, imgs, bandList=[band])\n",
    "    print(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} Mosaicd {tmp_vrt}\")\n",
    "    # write vrt mosaic to cog\n",
    "    convert2cog(ds, out_cog, 1)\n",
    "    ds = None\n",
    "\n",
    "    if os.path.exists(tmp_vrt):\n",
    "        os.remove(tmp_vrt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mosaic currently specific to pleiades ([:-13] for slicing name). Needs to be generalised a little more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def cogmosaicbands(img_paths, numbands, basename):\n",
    "    \"\"\"\n",
    "    Convert bands from img_paths to cog, output \n",
    "    basename_band#.tif. Mosaic if >1 img paths.\n",
    "    \"\"\"\n",
    "    if len(img_paths) == 1:\n",
    "        for b in range(1, numbands+1):\n",
    "            convert2cog(img_paths[0], f\"{basename}_band{b}.tif\", b)\n",
    "        for f in glob(f\"{img_paths[0][:-4]}*\"):\n",
    "            os.remove(f)\n",
    "    elif len(img_paths) > 1:\n",
    "        for b in range(1, numbands+1):\n",
    "            mosaic2cog(img_paths[0], f\"{basename}_band{b}.tif\", b)\n",
    "        for img_path in img_paths:\n",
    "            for f in glob(f\"{img_path[:-4]}*\"):\n",
    "                os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def s3_create_client(s3_bucket):\n",
    "    \"\"\"\n",
    "    Create and set up a connection to S3\n",
    "    :param s3_bucket:\n",
    "    :return: the s3 client object.\n",
    "    \"\"\"\n",
    "    access = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "    secret = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "\n",
    "    session = boto3.Session(\n",
    "        access,\n",
    "        secret,\n",
    "    )\n",
    "    endpoint_url = os.getenv(\"AWS_S3_ENDPOINT_URL\")\n",
    "\n",
    "    if endpoint_url is not None:\n",
    "        s3 = session.resource('s3', endpoint_url=endpoint_url)\n",
    "    else:\n",
    "        s3 = session.resource('s3', region_name='eu-west-2')\n",
    "\n",
    "    bucket = s3.Bucket(s3_bucket)\n",
    "\n",
    "    if endpoint_url is not None:\n",
    "        s3_client = boto3.client(\n",
    "            's3',\n",
    "            aws_access_key_id=access,\n",
    "            aws_secret_access_key=secret,\n",
    "            endpoint_url=endpoint_url\n",
    "        )\n",
    "    else:\n",
    "        s3_client = boto3.client(\n",
    "            's3',\n",
    "            aws_access_key_id=access,\n",
    "            aws_secret_access_key=secret\n",
    "        )\n",
    "    return s3_client, bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to connect to any S3-like object storage end point. Assumes env vars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "gb = 1024 ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def s3_single_upload(in_path, s3_path, s3_bucket):\n",
    "    \"\"\"\n",
    "    put a file into S3 from the local file system.\n",
    "    :param in_path: a path to a file on the local file system\n",
    "    :param s3_path: where in S3 to put the file.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # prep session & creds\n",
    "    s3_client, bucket = s3_create_client(s3_bucket)\n",
    "\n",
    "    # Ensure that multipart uploads only happen if the size of a transfer is larger than\n",
    "    # S3's size limit for non multipart uploads, which is 5 GB. we copy using multipart \n",
    "    # at anything over 4gb\n",
    "    transfer_config = boto3.s3.transfer.TransferConfig(multipart_threshold=2 * gb,\n",
    "                                                       max_concurrency=10,\n",
    "                                                       multipart_chunksize=2 * gb,\n",
    "                                                       use_threads=True)\n",
    "    s3_client.upload_file(in_path, bucket.name, s3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_rel_dir_s3_paths(local_dir, s3_dir):\n",
    "    \"\"\"\n",
    "    returns local path, remote path pair list.\n",
    "    \"\"\"  \n",
    "    paths = []\n",
    "    for subdir, dirs, files in os.walk(local_dir):\n",
    "        for file in files:\n",
    "            full_path = os.path.join(subdir, file)\n",
    "            paths.append([ full_path, s3_dir + local_dir.split('/')[-2] + '/' + full_path[len(local_dir):] ])\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def s3_upload_dir(in_dir, s3_bucket, s3_dir):\n",
    "    \"\"\"\n",
    "    Upload all items in directory, inc. dir.\n",
    "    \"\"\"\n",
    "    paths = get_rel_dir_s3_paths(in_dir, s3_dir)\n",
    "    upload_list = [(in_path, out_path, s3_bucket)\n",
    "                   for in_path, out_path in paths]\n",
    "    for i in upload_list:\n",
    "        s3_single_upload(i[0], i[1], i[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def s3_list_objects_paths(s3_bucket, prefix):\n",
    "    \"\"\"List of paths only returned, not full object responses\"\"\"\n",
    "    client, bucket = s3_create_client(s3_bucket)\n",
    "    \n",
    "    return [e['Key'] for p in client.get_paginator(\"list_objects_v2\").paginate(Bucket=s3_bucket, Prefix=prefix) for e in p['Contents']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def s3_list_objects(s3_bucket, prefix):\n",
    "    # prep session & creds\n",
    "    client, bucket = s3_create_client(s3_bucket)\n",
    "    response = client.list_objects_v2(Bucket=s3_bucket, Prefix=prefix)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def clean_up(work_dir):\n",
    "    # TODO: sort out logging changes...\n",
    "    gc.collect()\n",
    "    shutil.rmtree(work_dir)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def setup_logging():\n",
    "\n",
    "    logging.basicConfig(level=logging.DEBUG)\n",
    "    root = logging.getLogger()\n",
    "    root.setLevel(os.environ.get(\"LOGLEVEL\", \"DEBUG\"))\n",
    "\n",
    "    # Turn down rasterio. It is extremely chatty at debug level.\n",
    "    logging.getLogger(\"rasterio\").setLevel(\"INFO\")\n",
    "    logging.getLogger(\"rasterio._io\").setLevel(\"WARNING\")\n",
    "\n",
    "    # Boto Core is also very chatty at debug. Logging entire request text etc\n",
    "    logging.getLogger(\"botocore\").setLevel(\"INFO\")\n",
    "    logging.getLogger(\"boto\").setLevel(\"INFO\")\n",
    "    logging.getLogger(\"boto3.resources\").setLevel(\"INFO\")\n",
    "    logging.getLogger(\"s3transfer\").setLevel(\"INFO\")\n",
    "    logging.getLogger(\"urllib3\").setLevel(\"INFO\")\n",
    "\n",
    "    return root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STAC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can amend the default STAC I/O tools to work with our object storage.\n",
    "\n",
    "First up is to be able to create a uri given an endpoint, bucket name and object path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_uri(s3_path, s3_bucket='public-eo-data', endpoint_url=os.getenv('AWS_S3_ENDPOINT_URL')):    \n",
    "    return f\"{endpoint_url}/{s3_bucket}/{s3_path}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amend the reading method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def my_read_method(uri):\n",
    "    parsed = urlparse(uri)\n",
    "    if parsed.scheme == 's3':\n",
    "        bucket = parsed.netloc\n",
    "        key = parsed.path[1:]\n",
    "        s3 = boto3.resource('s3')\n",
    "        obj = s3.Object(bucket, key)\n",
    "        return obj.get()['Body'].read().decode('utf-8')\n",
    "    else:\n",
    "        return STAC_IO.default_read_text_method(uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the writing method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def my_write_method(uri, txt):\n",
    "    parsed = urlparse(uri)\n",
    "    if parsed.scheme == 's3':\n",
    "        bucket = parsed.netloc\n",
    "        key = parsed.path[1:]\n",
    "        s3 = boto3.resource(\"s3\")\n",
    "        s3.Object(bucket, key).put(Body=txt)\n",
    "    else:\n",
    "        STAC_IO.default_write_text_method(uri, txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then allow us to overwrite the defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def pystac_setIO():\n",
    "    STAC_IO.read_text_method = my_read_method\n",
    "    STAC_IO.write_text_method = my_write_method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_rediswq.ipynb.\n",
      "Converted 00_utils.ipynb.\n",
      "Converted 01A_pleiades.ipynb.\n",
      "Converted 01B_pleiades_prep_worker.ipynb.\n",
      "Converted 02A_spot.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
